{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "from models.binae import BinModel\n",
    "from einops import rearrange\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the configurations that you want to use to build your model\n",
    "\n",
    "Here, pick the desired configuration depending on the weights that you downloaded from our repository. The weights can be downloaded from; https://github.com/dali92002/DocEnTR in the Section Model Zoo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "THRESHOLD = 0.5  ## binarization threshold after the model output\n",
    "WORKERS = 4\n",
    "SPLITSIZE = 256  ## your image will be divided into patches of 256x256 pixels\n",
    "BATCH_SIZE = 4\n",
    "OUTPUT_PATH = Path(\"./demo/cleaned\")  #TODO: Add your desired output path here\n",
    "setting = \"base\"  ## choose the desired model size [small, base or large], depending on the model you want to use\n",
    "patch_size = 8  ## choose your desired patch size [8 or 16], depending on the model you want to use\n",
    "dataset_year = 2012\n",
    "image_size = (SPLITSIZE, SPLITSIZE)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if setting == 'base':\n",
    "    encoder_layers = 6\n",
    "    encoder_heads = 8\n",
    "    encoder_dim = 768\n",
    "\n",
    "elif setting == 'small':\n",
    "    encoder_layers = 3\n",
    "    encoder_heads = 4\n",
    "    encoder_dim = 512\n",
    "\n",
    "elif setting == 'large':\n",
    "    encoder_layers = 12\n",
    "    encoder_heads = 16\n",
    "    encoder_dim = 1024\n",
    "else:\n",
    "    raise ValueError(\"Invalid setting!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build your model\n",
    "Build the transformer autoencoder model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=1000,\n",
    "    dim=encoder_dim,\n",
    "    depth=encoder_layers,\n",
    "    heads=encoder_heads,\n",
    "    mlp_dim=2048\n",
    ")\n",
    "model = BinModel(\n",
    "    encoder=v,\n",
    "    decoder_dim=encoder_dim,\n",
    "    decoder_depth=encoder_layers,\n",
    "    decoder_heads=encoder_heads\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the pretrained weights that were downloaded from our repo:\n",
    "Here, give the path of the downloaded weights and load them to use a trained model."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = None  # TODO: Add the path to the model weights path here\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def showimg(img, torch_mode: bool = False):\n",
    "    if torch_mode:\n",
    "        img = torch.permute(img, (1, 2, 0))\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read the degraded image\n",
    "\n",
    "Here specify the location of the degraded image that you want to clean. and then read it. We also can visualize it before cleaning. Here I have a folder named demo that contain two folders named degraded and cleaned, the degraded contain the degraded image and the cleaned will contain later the obtained results. But you can specify your own names of folders."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from numpy.typing import ArrayLike\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "Height = int\n",
    "Width = int\n",
    "\n",
    "ImageShape = Tuple[Height, Width]\n",
    "\n",
    "\n",
    "class BinarisationDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fnames: List[Path],\n",
    "            imshape: Optional[ImageShape] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fnames = fnames\n",
    "        self.imshape = imshape\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Dict[str, Any]:\n",
    "        img, og_shape, rs_shape = self._load_image(self.fnames[item])\n",
    "        img = torch.tensor(img)\n",
    "        return {\n",
    "            \"img\": img,\n",
    "            \"og_shape\": og_shape,\n",
    "            \"rs_shape\": rs_shape,\n",
    "            \"fn_shape\": self.imshape,\n",
    "            \"fname\": self.fnames[item],\n",
    "        }\n",
    "\n",
    "    def _load_image(self, path: Path) -> Tuple[ArrayLike, ImageShape, ImageShape]:\n",
    "        img = cv2.imread(str(path))\n",
    "        og_shape = img.shape[0], img.shape[1]\n",
    "\n",
    "        if self.imshape is not None:\n",
    "            img, rs_shape = self._proportional_scaling(img)\n",
    "        else:\n",
    "            rs_shape = og_shape\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255  # height, width, 3\n",
    "\n",
    "        return img, og_shape, rs_shape\n",
    "\n",
    "    def _proportional_scaling(self, img: ArrayLike) -> Tuple[ArrayLike, ImageShape]:\n",
    "        \"\"\"Scale image to a set width and height with padding.\"\"\"\n",
    "        img_height, img_width, img_channels = img.shape\n",
    "\n",
    "        assert self.imshape is not None, \"No resizing should be done if shape is None\"\n",
    "        height, width = self.imshape\n",
    "\n",
    "        factor = min(height / img_height, width / img_width)\n",
    "        new_height, new_width = int(img_height * factor + 0.999), int(\n",
    "            img_width * factor + 0.999\n",
    "        )\n",
    "\n",
    "        img = cv2.resize(img, (new_width, new_height))\n",
    "        padded = np.zeros((*self.imshape, img_channels), dtype=img.dtype)\n",
    "        padded[:new_height, :new_width, :] = img\n",
    "\n",
    "        return padded, (new_height, new_width)\n",
    "\n",
    "    # You can implement your own dataset by writing a function that selects the images\n",
    "    # to clean and passes them to the constructor as a list.\n",
    "    @classmethod\n",
    "    def from_old_docs(cls, root_path: Path, imshape: Optional[ImageShape]) -> \"BinarisationDataset\":\n",
    "        return BinarisationDataset(list(root_path.glob(\"*/*.jpg\")), imshape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split and Merge functions\n",
    "\n",
    "Here, two function that are used for splitting an image into patches and merge a list of patches into an image. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PatchProcessor:\n",
    "    MEAN = torch.Tensor(np.array([0.485, 0.456, 0.406]))\n",
    "    STD = torch.Tensor(np.array([0.229, 0.224, 0.225]))\n",
    "\n",
    "    def __init__(self, nsize1: int, nsize2: int) -> None:\n",
    "        self.nsize1 = nsize1\n",
    "        self.nsize2 = nsize2\n",
    "\n",
    "    def split(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert image into a set of patches.\"\"\"\n",
    "        # im: h x w x 3\n",
    "        patches = rearrange(img, \"(h p1) (w p2) c -> (h w) p1 p2 c\", p1=self.nsize1, p2=self.nsize2, c=3)\n",
    "        return patches\n",
    "\n",
    "    def merge_image(self, patches: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "        \"\"\"Convert patches back into an image.\"\"\"\n",
    "        #patches: npatches x hpatch x wpatch x 3\n",
    "        img = rearrange(patches, \"(h w) p1 p2 c -> (h p1) (w p2) c\", p1=self.nsize1, p2=self.nsize2, c=3,\n",
    "                        h=h // self.nsize1, w=w // self.nsize2)\n",
    "        return img\n",
    "\n",
    "    def normalize_patches(self, patches: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply normalization on the color of the patches.\"\"\"\n",
    "        # patches: npatches x hpatch x wpatch x 3\n",
    "        patches = (patches - self.MEAN[None, None, None, :]) / self.STD[None, None, None, :]\n",
    "        return patches\n",
    "\n",
    "    def unnormalize_patches(self, patches: torch.Tensor):\n",
    "        # patches: npatches x hpatch x wpatch x 3\n",
    "        patches = (patches * self.STD[None, None, None, :]) + self.MEAN[None, None, None, :]\n",
    "        return patches\n",
    "\n",
    "    @staticmethod\n",
    "    def _closest_multiple(num: int, multiple: int) -> int:\n",
    "        return (math.ceil(num / multiple)) * multiple\n",
    "\n",
    "    def prepare_image(self, image: torch.Tensor):\n",
    "        \"\"\"Split the image intop patches, an image is padded first to make it dividable by the split size.\"\"\"\n",
    "        # image: h x w x 3\n",
    "        h = self._closest_multiple(image.shape[0], self.nsize1)\n",
    "        w = self._closest_multiple(image.shape[1], self.nsize2)\n",
    "\n",
    "        image_padded = torch.ones((h, w, 3))\n",
    "        image_padded[:image.shape[0], :image.shape[1], :] = image\n",
    "        patches = self.split(image_padded)\n",
    "        patches = self.normalize_patches(patches)\n",
    "        # patches: npatches x hpatch x wpatch x 3\n",
    "        return patches\n",
    "\n",
    "    def unprepare_image(self, patches, rs_shape: ImageShape, fn_shape: ImageShape) -> torch.Tensor:\n",
    "        # patches: npatches x hpatch x wpatch x 3\n",
    "        h = self._closest_multiple(fn_shape[0], self.nsize1)\n",
    "        w = self._closest_multiple(fn_shape[1], self.nsize2)\n",
    "\n",
    "        patches = self.unnormalize_patches(patches)\n",
    "        image = self.merge_image(patches, h, w)\n",
    "\n",
    "        # image: h' x w' x 3\n",
    "        image = image[:rs_shape[0], :rs_shape[1], :]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class PatchCollator:\n",
    "    def __init__(self):\n",
    "        self.processor = PatchProcessor(SPLITSIZE, SPLITSIZE)\n",
    "\n",
    "    def get_processor(self) -> PatchProcessor:\n",
    "        return self.processor\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        output = {\n",
    "            \"patches\": [],\n",
    "            \"npatches\": [],\n",
    "            \"end_indices\": [],\n",
    "            \"fnames\": [],\n",
    "            \"og_shapes\": [],\n",
    "            \"rs_shapes\": [],\n",
    "            \"fn_shapes\": [],\n",
    "        }\n",
    "        for sample in batch:\n",
    "            img, og_shape, rs_shape, fn_shape, fname = sample[\"img\"], sample[\"og_shape\"], sample[\"rs_shape\"], sample[\n",
    "                \"fn_shape\"], sample[\"fname\"]\n",
    "            patches = self.processor.prepare_image(img)\n",
    "            output[\"patches\"].append(patches)\n",
    "            output[\"npatches\"].append(len(patches))\n",
    "            output[\"fnames\"].append(fname)\n",
    "            output[\"og_shapes\"].append(og_shape)\n",
    "            output[\"rs_shapes\"].append(rs_shape)\n",
    "            output[\"fn_shapes\"].append(fn_shape)\n",
    "            output[\"end_indices\"].append(\n",
    "                len(patches) if len(output[\"end_indices\"]) == 0 else output[\"end_indices\"][-1] + len(patches))\n",
    "        output[\"patches\"] = torch.cat(output[\"patches\"], dim=0)\n",
    "        return output\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "collator = PatchCollator()\n",
    "dataset = BinarisationDataset(list(Path(\"./demo/degraded/\").glob(\"*\")), (1024, 1024))  # TODO: Change your dataset here\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator.collate_fn,\n",
    "                        num_workers=WORKERS, pin_memory=False, drop_last=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "showimg(dataset[2][\"img\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean your image with the trained model\n",
    "\n",
    "Clean your list of patches obe by one."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def export_image(pred_patches: torch.Tensor, batch: Dict[str, Any]) -> None:\n",
    "    prev = 0\n",
    "    for jj, (end_ind, og_shape, rs_shape, fn_shape, fname) in enumerate(\n",
    "            zip(batch[\"end_indices\"], batch[\"og_shapes\"], batch[\"rs_shapes\"], batch[\"fn_shapes\"], batch[\"fnames\"])):\n",
    "        img_patches = pred_patches[prev: end_ind]\n",
    "        img_clean = processor.unprepare_image(img_patches, rs_shape, fn_shape)\n",
    "        prev = end_ind\n",
    "        img_clean = (img_clean.numpy() > THRESHOLD) * 255\n",
    "        img_clean = img_clean.astype(np.uint8)\n",
    "        cv2.imwrite(str(OUTPUT_PATH / f\"{fname.stem}.png\"), cv2.cvtColor(img_clean, cv2.COLOR_RGB2BGR))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "thread_pool = ThreadPool(8 - WORKERS)\n",
    "result = []\n",
    "processor = collator.get_processor()\n",
    "\n",
    "for ii, batch in enumerate(dataloader):\n",
    "    train_in = batch[\"patches\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_in = train_in.to(device)\n",
    "        train_in = train_in.permute(0, 3, 1, 2)  # b, ch, h, w\n",
    "        any_img = torch.rand(train_in.shape).to(device)\n",
    "        _, _, pred_patches = model(train_in, any_img)\n",
    "        pred_patches = torch.squeeze(\n",
    "            rearrange(pred_patches, 'b (h w) (p1 p2 c) -> b (h p1) (w p2) c', p1=patch_size, p2=patch_size,\n",
    "                      h=image_size[0] // patch_size))\n",
    "        pred_patches = pred_patches.detach().cpu()\n",
    "        export_thread = thread_pool.apply(export_image, [pred_patches, batch])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d9f0b2f7ce66eb0711cec7c8aaa7059f0173343c6ccde03c0c9a786f97f4034"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
